\section{Rice Varieties}

\subsection{Preliminary Observations}

\subsubsection{Dataset}

All following considerations are made using the datasets provided on the challenge page (\textit{rice\_test.csv} and \textit{rice\_train.csv}), but it is worth noting that they both stem from an original one that can be found at the \href{https://www.muratkoklu.com/datasets/}{dataset source page}. I used it to compute an alternative version of the test dataset that contains the true attribute for \textit{Class} (listing \ref{code:mergedatasets}).

\begin{lstlisting}[language={Python},label={code:mergedatasets}, caption={Merge datasets}]
df1 = pd.read_csv("rice_test.csv").round(10)
df2 = pd.read_csv("Rice_Cammeo_Osmancik.csv").round(10)

keys = [col for col in df1.columns]
merged = df1.merge(df2, on=keys, how="inner")
\end{lstlisting}

\subsubsection{Scatter Plot}

From the \textit{scatter plot} (figure \ref{fig:scatterplot_rice_loess}) we can infer that there are four features (\textit{Area}, \textit{Perimeter}, \textit{Convex\_Area} and \textit{Major\_Axis\_Length}) that seem to be extremely correlated

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{imgs/scatterplot_rice_loess.png}
  \caption{Scatter Plot for the \textit{rice\_train} dataset with LOESS smoothing lines for each class.}
    \Description{Scatterplot of a multivariate dataset.}
  \label{fig:scatterplot_rice_loess}
\end{figure}

\subsubsection{Correlation Matrix}

We can use a correlation matrix (table \ref{tab:correlation_matrix_rice}) to see exactly how related to each other the features are. As suspected, all four features before-mentioned have a high degree of correlation (over ninety percent). Once way to handle this is to either use models robust against collinearity, or to either combine or remove some of the correlated features.  

\begin{table}
  \small
  \centering
  \caption{Correlation matrix of features}
  \label{tab:correlation_matrix_rice}
  \begin{tabularx}{\textwidth}{lXXXXXXX}
    \toprule 
     & Area & Perimeter & Major Axis Length & Minor Axis Length & Eccentricity & Convex Area & Extent \\
    \midrule
    Area              &  1.00000000 &  0.96704011 &  0.90356325 &  0.79022701 &  0.34653177 &  0.99895272 & -0.06002223 \\
    Perimeter         &  0.96704011 &  1.00000000 &  0.97163180 &  0.63486482 &  0.53784650 &  0.97046788 & -0.12718015 \\
    Major Axis Length &  0.90356325 &  0.97163180 &  1.00000000 &  0.45675159 &  0.70625660 &  0.90390912 & -0.13562592 \\
    Minor Axis Length &  0.79022701 &  0.63486482 &  0.45675159 &  1.00000000 & -0.29393931 &  0.78975480 &  0.06192558 \\
    Eccentricity      &  0.34653177 &  0.53784650 &  0.70625660 & -0.29393931 &  1.00000000 &  0.34707340 & -0.19301157 \\
    Convex Area       &  0.99895272 &  0.97046788 &  0.90390912 &  0.78975480 &  0.34707340 &  1.00000000 & -0.06397213 \\
    Extent            & -0.06002223 & -0.12718015 & -0.13562592 &  0.06192558 & -0.19301157 & -0.06397213 &  1.00000000 \\
    \bottomrule
  \end{tabularx}
\end{table}

\subsubsection{Features' Importance}

To get a better idea on the importance of each feature, I trained a simple linear regression model, and looked at the result with \verb|summary(fit)|, which can be seen in table \ref{tab:regression_coefficients}. The significance stars tells us visually that, with the exception of \textit{Minor\_Axis\_Length}, the features that are highly correlated to each other contributes strongly to the model, while the \textit{Eccentricity} and \textit{Extent} contribute very little.

\begin{table}
  \centering
  \caption{Regression coefficients and coefficients' importance}
  \label{tab:regression_coefficients}
  \begin{tabularx}{\textwidth}{lXXXXX}
    \toprule
    Parameter & Estimate & Std. Error & t value & Pr(>|t|) & Importance\\
    \midrule
    (Intercept)       & -2.152e+00 & 1.990e+00 & -1.081 & 0.279633 &  \\
    Area              &  5.601e-04 & 1.023e-04 &  5.474 & 4.79e-08 & *** \\
    Perimeter         &  8.440e-03 & 2.221e-03 &  3.800 & 0.000148 & *** \\
    Major\_Axis\_Length & -2.198e-02 & 5.709e-03 & -3.851 & 0.000120 & *** \\
    Minor\_Axis\_Length &  4.669e-02 & 1.213e-02 &  3.850 & 0.000121 & *** \\
    Eccentricity      &  4.534e+00 & 1.828e+00 &  2.481 & 0.013170 & * \\
    Convex\_Area       & -8.609e-04 & 9.418e-05 & -9.141 & < 2e-16 & *** \\
    Extent            &  7.146e-02 & 7.144e-02 &  1.000 & 0.317237 &  \\
    \bottomrule
  \end{tabularx}
\end{table}

We can also measure the total variance explained by all predictors combined using $R^2 = 0.6953849$, and we can check the standardized coefficients to better compare predictors' importance, as shown in table \ref{tab:standardized_coefficients}. Once again, we can see that Extent and Eccentricity contribute very little to the overall model.

\begin{table}
  \centering
  \caption{Standardized regression coefficients}
  \label{tab:standardized_coefficients}
  \begin{tabularx}{0.45\textwidth}{lc}
    \toprule
    Parameter & Standardized Coefficient \\
    \midrule
    (Intercept)       & NA \\
    Area              & 1.95970669 \\
    Perimeter         & 0.60605955 \\
    Major\_Axis\_Length & -0.77272038 \\
    Minor\_Axis\_Length & 0.54256082 \\
    Eccentricity      & 0.18931314 \\
    Convex\_Area       & -3.09099064 \\
    Extent            & 0.01114328 \\
    \bottomrule
  \end{tabularx}
\end{table}

\subsubsection{Principal Components}

A good way to aggregate and simplify data is by decomposing it into its principal components (centered in zero and scaled to have unit variance). Components' summary can be seen in table \ref{tab:pca_importance}, and it shows us that more than ninety nine percent of overall variance can be explained with just three components, so a 2D visualization with only the first two components should give us a good idea of the whole dataset (figure \ref{fig:2pca_rice}). The plot suggests that data is well separated, with similar classes clustering nicely. This should make classification easier. 

\begin{table}
  \centering
  \caption{Summary of principal components}
  \label{tab:pca_importance}
  \begin{tabularx}{\textwidth}{lXXXXXXXX}
    \toprule
    & PC1 & PC2 & PC3 & PC4 & PC5 & PC6 & PC7 & PC8 \\
    \midrule
    Standard deviation      & 2.2924  & 1.2436  & 0.9562  & 0.51393 & 0.10621 & 0.07758 & 0.04519 & 0.02052 \\
    Proportion of Variance  & 0.6569  & 0.1933  & 0.1143  & 0.03302 & 0.00141 & 0.00075 & 0.00026 & 0.00005 \\
    Cumulative Proportion   & 0.6569  & 0.8502  & 0.9645  & 0.99753 & 0.99894 & 0.99969 & 0.99995 & 1.00000 \\
    \bottomrule
  \end{tabularx}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{imgs/pca_plot_rice.png}
  \caption{Plot with first two principal components of \textit{rice\_train} dataset.}
    \Description{PC1 PC2 plot for rice dataset.}
  \label{fig:2pca_rice}
\end{figure}

\subsection{Modeling}

After exploring the dataset, I tried to find the best performing model for predicting the target.

\subsubsection{Linear Regression}

First I split the training dataset into training and validation sets, fitted the model and compute its \textit{MSE}. From this basic regression I got $MSE = 0.07354557$ and $RMSE = 0.2711929$. Since we know from the preliminary observations that some features are highly correlated (so the dataset suffers from collinearity), I tried to transform the dataset to reduce it. A straightforward solution is to compute the average of all of them (listing \ref{code:avg_rice_correlated}), but this yield a slightly worse result: $MSE \approx 0.074$.

Another approach would be to drop features altogether, but first we want to measure their variance inflation factor. As a rule of thumb, a \textit{VIF} value above then indicates a problematic amount of collinearity, so I tired to drop features iteratively to see if the model improves, as shown in table \ref{tab:vif_drop_analysis}. For each dropped variable I computed the \textit{VIFs} and the \textit{MSE} of the resulting model. As we can see, dropping \textit{Area} resulted in performances similar to combining the variables, while dropping the other features worsened prediction performances even further.

\begin{table}
  \small
  \centering
  \caption{Effect of variable removal on variance inflation factors (VIF) and mean squared error (MSE)}
  \label{tab:vif_drop_analysis}
  \begin{tabularx}{\textwidth}{lccccccc}
    \toprule
    Variable Dropped & VIF Perimeter & VIF Major\_A\_L & VIF Minor\_A\_L & VIF Ecc. & VIF Convex\_A & VIF Extent & MSE \\
    \midrule
    Area              & 114 & 206 & 133 & 53 & 332 & 1 & 0.0735629 \\
    Convex Area       & 108 & 134 & 40  & 49 & --  & 1 & 0.07572877 \\
    Major Axis Length & 47  & --  & 37  & 29 & --  & 1 & 0.07718435 \\
    Perimeter         & --  & --  & 1   & 1  & --  & 1 & 0.0806868 \\
    \bottomrule
  \end{tabularx}
\end{table}

\begin{lstlisting}[language={R},label={code:avg_rice_correlated}, caption={Average of highly correlated features}]
rice.train$Combined <- rowMeans(rice.train[, c("Area","Perimeter","Major_Axis_Length","Convex_Area")])

linear_fit = lm(Class ~ 
                Minor_Axis_Length
                + Eccentricity
                + Extent
                + Combined, 
                data=rice_train, 
                subset = train
                )

pred <- predict(linear_fit, newdata = rice_train[test, ])
mean((pred - y.test)^2)     # [1] 0.07439239
\end{lstlisting}

\subsubsection{Ridge Regression}