\section{Rice Varieties}

\subsection{Preliminary Observations}

\subsubsection{Dataset}

All following considerations are made using the datasets provided on the challenge page (\textit{rice\_test.csv} and \textit{rice\_train.csv}), but it is worth noting that they both stem from an original one that can be found at the \href{https://www.muratkoklu.com/datasets/}{dataset source page}. I used it to compute an alternative version of the test dataset that contains the true attribute for \textit{Class} (listing \ref{code:mergedatasets}).

\begin{lstlisting}[language={Python},label={code:mergedatasets}, caption={Merge datasets}]
df1 = pd.read_csv("rice_test.csv").round(10)
df2 = pd.read_csv("Rice_Cammeo_Osmancik.csv").round(10)

keys = [col for col in df1.columns]
merged = df1.merge(df2, on=keys, how="inner")
\end{lstlisting}

\subsubsection{Scatter Plot}

From the \textit{scatter plot} (figure \ref{fig:scatterplot_rice_loess}) we can infer that there are four features (\textit{Area}, \textit{Perimeter}, \textit{Convex\_Area} and \textit{Major\_Axis\_Length}) that seem to be extremely correlated

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{imgs/scatterplot_rice_loess.png}
  \caption{Scatter Plot for the \textit{rice\_train} dataset with LOESS smoothing lines for each class.}
    \Description{Scatterplot of a multivariate dataset.}
  \label{fig:scatterplot_rice_loess}
\end{figure}

\subsubsection{Correlation Matrix}

We can use a correlation matrix (table \ref{tab:correlation_matrix_rice}) to see exactly how related to each other the features are. As suspected, all four features before-mentioned have a high degree of correlation (over ninety percent). Once way to handle this is to either use models robust against collinearity, or to either combine or remove some of the correlated features.  

\begin{table}
  \small
  \centering
  \caption{Correlation matrix of features}
  \label{tab:correlation_matrix_rice}
  \begin{tabularx}{\textwidth}{lXXXXXXX}
    \toprule 
     & Area & Perimeter & Major Axis Length & Minor Axis Length & Eccentricity & Convex Area & Extent \\
    \midrule
    Area              &  1.00000000 &  0.96704011 &  0.90356325 &  0.79022701 &  0.34653177 &  0.99895272 & -0.06002223 \\
    Perimeter         &  0.96704011 &  1.00000000 &  0.97163180 &  0.63486482 &  0.53784650 &  0.97046788 & -0.12718015 \\
    Major Axis Length &  0.90356325 &  0.97163180 &  1.00000000 &  0.45675159 &  0.70625660 &  0.90390912 & -0.13562592 \\
    Minor Axis Length &  0.79022701 &  0.63486482 &  0.45675159 &  1.00000000 & -0.29393931 &  0.78975480 &  0.06192558 \\
    Eccentricity      &  0.34653177 &  0.53784650 &  0.70625660 & -0.29393931 &  1.00000000 &  0.34707340 & -0.19301157 \\
    Convex Area       &  0.99895272 &  0.97046788 &  0.90390912 &  0.78975480 &  0.34707340 &  1.00000000 & -0.06397213 \\
    Extent            & -0.06002223 & -0.12718015 & -0.13562592 &  0.06192558 & -0.19301157 & -0.06397213 &  1.00000000 \\
    \bottomrule
  \end{tabularx}
\end{table}

\subsubsection{Features' Importance}

To get a better idea on the importance of each feature, I trained a simple linear regression model, and looked at the result with \verb|summary(fit)|, which can be seen in table \ref{tab:regression_coefficients}. The significance stars tells us visually that, with the exception of \textit{Minor\_Axis\_Length}, the features that are highly correlated to each other contributes strongly to the model, while the \textit{Eccentricity} and \textit{Extent} contribute very little.

\begin{table}
  \centering
  \caption{Regression coefficients and coefficients' importance}
  \label{tab:regression_coefficients}
  \begin{tabularx}{\textwidth}{lXXXXX}
    \toprule
    Parameter & Estimate & Std. Error & t value & Pr(>|t|) & Importance\\
    \midrule
    (Intercept)       & -2.152e+00 & 1.990e+00 & -1.081 & 0.279633 &  \\
    Area              &  5.601e-04 & 1.023e-04 &  5.474 & 4.79e-08 & *** \\
    Perimeter         &  8.440e-03 & 2.221e-03 &  3.800 & 0.000148 & *** \\
    Major\_Axis\_Length & -2.198e-02 & 5.709e-03 & -3.851 & 0.000120 & *** \\
    Minor\_Axis\_Length &  4.669e-02 & 1.213e-02 &  3.850 & 0.000121 & *** \\
    Eccentricity      &  4.534e+00 & 1.828e+00 &  2.481 & 0.013170 & * \\
    Convex\_Area       & -8.609e-04 & 9.418e-05 & -9.141 & < 2e-16 & *** \\
    Extent            &  7.146e-02 & 7.144e-02 &  1.000 & 0.317237 &  \\
    \bottomrule
  \end{tabularx}
\end{table}

We can also measure the total variance explained by all predictors combined using $R^2 = 0.6953849$, and we can check the standardized coefficients to better compare predictors' importance, as shown in table \ref{tab:standardized_coefficients}. Once again, we can see that Extent and Eccentricity contribute very little to the overall model.

\begin{table}
  \centering
  \caption{Standardized regression coefficients}
  \label{tab:standardized_coefficients}
  \begin{tabularx}{0.45\textwidth}{lc}
    \toprule
    Parameter & Standardized Coefficient \\
    \midrule
    (Intercept)       & NA \\
    Area              & 1.95970669 \\
    Perimeter         & 0.60605955 \\
    Major\_Axis\_Length & -0.77272038 \\
    Minor\_Axis\_Length & 0.54256082 \\
    Eccentricity      & 0.18931314 \\
    Convex\_Area       & -3.09099064 \\
    Extent            & 0.01114328 \\
    \bottomrule
  \end{tabularx}
\end{table}

\subsubsection{Principal Components}

A good way to aggregate and simplify data is by decomposing it into its principal components (centered in zero and scaled to have unit variance). Components' summary can be seen in table \ref{tab:rice_pca_importance}, and it shows us that more than ninety nine percent of overall variance can be explained with just three components, so a 2D visualization with only the first two components should give us a good idea of the whole dataset (figure \ref{fig:rice_2pca_plot}). The plot suggests that data is well separated, with similar classes clustering nicely. This should make classification easier. 

\begin{table}
  \centering
  \caption{Summary of principal components}
  \label{tab:rice_pca_importance}
  \begin{tabularx}{\textwidth}{lXXXXXXXX}
    \toprule
    & PC1 & PC2 & PC3 & PC4 & PC5 & PC6 & PC7 & PC8 \\
    \midrule
    Standard deviation      & 2.2924  & 1.2436  & 0.9562  & 0.51393 & 0.10621 & 0.07758 & 0.04519 & 0.02052 \\
    Proportion of Variance  & 0.6569  & 0.1933  & 0.1143  & 0.03302 & 0.00141 & 0.00075 & 0.00026 & 0.00005 \\
    Cumulative Proportion   & 0.6569  & 0.8502  & 0.9645  & 0.99753 & 0.99894 & 0.99969 & 0.99995 & 1.00000 \\
    \bottomrule
  \end{tabularx}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{imgs/pca_plot_rice.png}
  \caption{Plot with first two principal components of \textit{rice\_train} dataset.}
    \Description{PC1 PC2 plot for rice dataset.}
  \label{fig:rice_2pca_plot}
\end{figure}

\subsection{Modeling}

After exploring the dataset, I tried to find the best performing model for predicting the target.

\subsubsection{Linear Regression}

First I split the training dataset into training and validation sets, fitted the model and compute its \textit{MSE}. From this basic regression I got $MSE = 0.07354557$. Since we know from the preliminary observations that some features are highly correlated (so the dataset suffers from collinearity), I tried to transform the dataset to reduce it. A straightforward solution is to compute the average of all of them (listing \ref{code:avg_rice_correlated}), but this yield a slightly worse result: $MSE \approx 0.074$.

Another approach would be to drop features altogether, but first we want to measure their variance inflation factor. As a rule of thumb, a \textit{VIF} value above then indicates a problematic amount of collinearity, so I tired to drop features iteratively to see if the model improves, as shown in table \ref{tab:vif_drop_analysis}. For each dropped variable I computed the \textit{VIFs} and the \textit{MSE} of the resulting model. As we can see, dropping \textit{Area} resulted in performances similar to combining the variables, while dropping the other features worsened prediction performances even further.

\begin{table}
  \small
  \centering
  \caption{Effect of variable removal on variance inflation factors (VIF) and mean squared error (MSE)}
  \label{tab:vif_drop_analysis}
  \begin{tabularx}{\textwidth}{lccccccc}
    \toprule
    Variable Dropped & VIF Perimeter & VIF Major\_A\_L & VIF Minor\_A\_L & VIF Ecc. & VIF Convex\_A & VIF Extent & MSE \\
    \midrule
    Area              & 114 & 206 & 133 & 53 & 332 & 1 & 0.0735629 \\
    Convex Area       & 108 & 134 & 40  & 49 & --  & 1 & 0.07572877 \\
    Major Axis Length & 47  & --  & 37  & 29 & --  & 1 & 0.07718435 \\
    Perimeter         & --  & --  & 1   & 1  & --  & 1 & 0.0806868 \\
    \bottomrule
  \end{tabularx}
\end{table}

\begin{lstlisting}[language={R},label={code:avg_rice_correlated}, caption={Average of highly correlated features}]
rice.train$Combined <- rowMeans(rice.train[, c("Area","Perimeter","Major_Axis_Length","Convex_Area")])

linear_fit = lm(Class ~ 
                Minor_Axis_Length
                + Eccentricity
                + Extent
                + Combined, 
                data=rice_train, 
                subset = train
                )

pred <- predict(linear_fit, newdata = rice_train[test, ])
mean((pred - y.test)^2)     # [1] 0.07439239
\end{lstlisting}

\subsubsection{Ridge Regression}

I used the \textit{glmnet} library, which takes care of the variable standardization for us, has a built-in cross-validation functionionality to compute the best possible $\lambda$. With code \ref{code:rice_ridge} we get a result worse than linear regression, since $MSE = 0.07924703$. That being said, we can see in plot \ref{fig:rice_ridge} an effective shrinkage of most coefficients towards zero, with the exception of Eccentricity, which we already recognized as not very impactful on the overall variance and prediction.

\begin{lstlisting}[language={R},label={code:rice_ridge}, caption={Ridge regression on rice dataset}]
library(glmnet)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
cv_mse <- min(cv.out$cvm)     # [1] 0.07924703
cv_rmse <- sqrt(cv_mse)       # [1] 0.2815085
\end{lstlisting}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{imgs/ridgeplot.png}
  \caption{Ridge coefficients for \textit{rice\_train} dataset.}
    \Description{Plot shows coefficient shrinkage for ridge regression.}
  \label{fig:rice_ridge}
\end{figure}

\subsubsection{Lasso Regression}

Very similar to ridge regression, except \textit{glmnet} gets called with \verb|alpha = 1|. We get a slightly better result than ridge, with $MSE = 0.07348723$. From the plot (in figure \ref{fig:rice_lasso}), we can see that once again Eccentricity seems to resist shrinkage, but overall coefficient reduction is much more accentuated compare to ridge, since lasso can set them to zero. 

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{imgs/lassoplot.png}
  \caption{Lasso coefficients for \textit{rice\_train} dataset.}
    \Description{Plot shows coefficient shrinkage for lasso regression.}
  \label{fig:rice_lasso}
\end{figure}

\subsubsection{Principal Components Regression}

I fitted the model with \textit{pls} library built-in cross-validation, as seen in code \ref{code:rice_pcr}. The lowest error (figure \ref{fig:rice_pcr}) is achieved with seven components: $MSE =  0.07354557$, slightly worse than lasso regression.

\begin{lstlisting}[language={R},label={code:rice_pcr}, caption={Principal components regression on rice dataset}]
pcr.fit <- pcr(Class ~ ., 
              data = rice_train, 
              subset = train, 
              scale = TRUE, 
              validation = "CV"
              )
\end{lstlisting}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{imgs/validationplot_pcr.png}
  \caption{PCR cross-validation MSE error for target \textit{Class} on \textit{rice\_train} dataset.}
    \Description{Plot shows MSE getting lower as components are added.}
  \label{fig:rice_pcr}
\end{figure}

\subsubsection{Partial Least Squares}

I used the same \textit{pls} library to fit the partial least squares model on the dataset. The best result is obtained with just two components (figure \ref{fig:rice_pls}), yielding $MSE = 0.07609556$, which is slightly worse than \textit{PCR} but needs a third of the components, potentially leading to less overfitting and computational cost.

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{imgs/validationplot_pls.png}
  \caption{PLS cross-validation MSE error for target \textit{Class} on \textit{rice\_train} dataset.}
    \Description{Plot shows MSE getting lower as components are added.}
  \label{fig:rice_pls}
\end{figure}

\subsubsection{LOESS}

I then tried some non-parametric methods, starting with \textit{LOESS} regression. This method allows up to four predictors, so I used the most useful ones previously identified: \textit{Area}, \textit{Perimeter}, \textit{Convex\_Area} and \textit{Major\_Axis\_Length}. Predictors are normalized by default. Then, fitted three models with three different values for \verb|span|. Predictions must be sanitized since there some test points can fall outside the local neighborhood, meaning being \verb|Na|. With $span = 0.1$ we have five such predictions. The code can be seen at \ref{code:rice_loess}. The results were disastrous, so I tried to include only two predictors. This yielded the best results yet out of all models: with $span = 0.1$, $MSE=0.06099749$. All results are summarized in table \ref{tab:rice_loess}

\begin{lstlisting}[language={R},label={code:rice_loess}, caption={LOESS regression on rice dataset with four predictors and $span=1$}]
loess_fit1 <- loess(Class ~ Area 
                    + Perimeter 
                    + Convex_Area 
                    + Major_Axis_Length, 
                    data = rice_train, 
                    subset = train, 
                    span = 0.1
                    )

pred1 <- predict(loess_fit1, newdata = rice_train[test, ])
mean((pred1 - y.test)^2, na.rm = TRUE)    # [1] 46.0226
\end{lstlisting}

\begin{table}
  \centering
  \caption{MSE for LOESS with different \textit{span} values and number of predictors}
  \label{tab:rice_loess}
  \begin{tabularx}{.4\textwidth}{ccc}
    \toprule
    Span & Number of Predictors & MSE \\
    \midrule
    0.1 & Four (4) & 46.0226 \\
    0.5 & Four (4) & 1.799422 \\
    0.9 & Four (4) & 0.2276324 \\
    \midrule
    0.1 & Two (2) & 0.06099749 \\
    0.5 & Two (2) & 0.06514592 \\
    0.9 & Two (2) & 0.06836381 \\
    \bottomrule
  \end{tabularx}
\end{table}

\subsubsection{K-Nearest Neighbors}