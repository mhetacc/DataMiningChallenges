\section{Phone Users}

\subsection{Preliminary Observations}

\subsubsection{Dataset}

Let's first briefly analyze the dataset.

\begin{itemize}
\small
  \item \textbf{For each user:}
  \begin{itemize}
    \item Plan type
    \item Payment method
    \item Sex
    \item Activation zone
    \item Activation channel
    \item Value-added service 1
    \item Value-added service 2
    \item \textbf{For each month:}
    \begin{itemize}
      \item $|\text{expensive calls}|$
      \item $|\text{cheap calls}|$
      \item $\text{time}(\text{expensive calls})$
      \item $\text{time}(\text{cheap calls})$
      \item $\text{cost}(\text{expensive calls})$
      \item $\text{cost}(\text{cheap calls})$
      \item $|\text{incoming calls}|$
      \item $\text{time}(\text{incoming calls})$
      \item $|\text{SMS}|$
      \item $|\text{calls to call center}|$
    \end{itemize}
  \end{itemize}
\end{itemize}

Considering that the target is monthly call time we can make the hypothesis that some features do not interest us, for example any monthly feature not concerning with call time itself, leaving us with only $time(expensive calls)$ and $time(cheap calls)$, which we could further assume can be combined since overall cost is not a concern. It goes without saying that all of the above will have to be be proven empirically.

\subsubsection{Scatter Plot}

I filtered out all not call time-related monthly data. The result (figure \ref{fig:phone_scatter_full}) is a bit hard to read due to its sheer size. Looking at it more up close we can infer some things: first of all we see that call data looks positively skewed (figure \ref{fig:phone_scatter_call}), and that there are some activation channels that see more call time than others, while activation regions seem to differ lightly between one another (figure \ref{fig:phone_scatter_geog+channel}). On the other hand, plan and sex seem to have an impact on call time (figure \ref{fig:phone_scatter_plan+sex}), while age does not seem to have a big impact, with the exception of the very young and very old (figure \ref{fig:phone_scatter_age}).

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\linewidth]{imgs/scatterplot_phone_full_20perc.png}
  \caption{Scatter Plot for the filtered \textit{phone\_train} dataset.}
    \Description{Scatterplot of a multivariate dataset.}
  \label{fig:phone_scatter_full}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{imgs/scatter_calldata.png}
  \caption{Scatter Plot for call time for \textit{phone\_train} dataset.}
    \Description{Scatterplot of multivariate call time.}
  \label{fig:phone_scatter_call}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=\linewidth]{imgs/scatter_geog+channel_overcalltime.png}
        \caption{Activation zone (X-asis, L) and activation channel (X-axis, R) over call time (Y axis)}
        \label{fig:phone_scatter_geog+channel}
    \end{subfigure}
    \hspace{.005\linewidth}
    \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=\linewidth]{imgs/scatter_plan+sex_over_calltime.png}
        \caption{Plan type (X-axis, L) and sex (X-axis, R) over call time (Y axis)}
        \label{fig:phone_scatter_plan+sex}
    \end{subfigure}
        \hspace{.005\linewidth}
    \begin{subfigure}{0.175\textwidth}
        \includegraphics[width=\linewidth]{imgs/scatter_age_over_calltime.png}
        \caption{Age (X-axis) over call time (Y-axis)}
        \label{fig:phone_scatter_age}
    \end{subfigure}

    \caption{Different features plotted over call time for the \textit{phone\_train} dataset.}
    \Description{Many multivariate graphs showing different features plotted over call time.}
    \label{fig:phone_scatter_allovercalls}
\end{figure}

\subsubsection{Call Features Over Everything Else}

Let's try to see if calls amount and time are related to other features.
First I plotted total calls taken and total calls time over nine months (figure \ref{fig:phone_calltimesandamount}). We can see that the graphs are almost identical from a trend standpoint, hinting at a strong correlation between number of calls and time spent calling. 

Then I wanted to see whether sex plays any role in total call time. Using the library \textit{dplyr} I computed all data in table \ref{tab:calltime_sex_cuts}. On average, men's calls seem to last 8\% longer. To mitigate the effect of outliers I filtered out the top 1\%, which still shows the same trend, albeit reduced to 5\%. Cutting even further the top 10\% shows a bigger increase in men's average call times of about 10\%. Lastly, I tired to log-transform the data. Even in this situation we see that men have longer calls, hinting that this predictor may be useful. 
I did exactly the same with payment methods, activation zones, activation channels and both value-added services, always cutting off the top 1\%,. All data can be seen in table \ref{tab:calltime_over_all}. There seem to be quite a bit of variance in call time compared to the tariff plan, which would make sense: some plans may be geared toward calling, while others are more suited sending SMS. We can see some variance on payment method too. Activation zone and activation channel seem to have some influence on call times, with the exception of zone eight, whose call times are way lower. On the other hand, customers that have activated either first or second added-value services seem to do longer calls.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{imgs/monthly_calls.png}
        \caption{Total amount of calls per month}
    \end{subfigure}
    \hspace{.05\linewidth}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{imgs/monthly_calltime.png}
        \caption{Total call time per month}
    \end{subfigure}

    \caption{Total calls and total call time per month for the \textit{phone\_train} dataset.}
    \Description{Graphs showing total calls and total call time per month.}
    \label{fig:phone_calltimesandamount}
\end{figure}

\begin{table}
  \centering
  \caption{Call time statistics by sex under different data cuts and transformations}
  \label{tab:calltime_sex_cuts}
  \begin{tabularx}{.75\textwidth}{lcccc}
    \toprule
    Type of cut & Sex & Number of customers & Total call time & Average call time \\
    \midrule
    None (raw)      & B & 5266 & 557266412 & 105823 \\
    None (raw)      & F & 1030 &  62493290 &  60673 \\
    None (raw)      & M & 3704 & 242226228 &  65396 \\
    \midrule
    Top 1\% cut     & B & 5201 & 498846876 &  95914 \\
    Top 1\% cut     & F & 1025 &  57203392 &  55808 \\
    Top 1\% cut     & M & 3674 & 215801665 &  58738 \\
    \midrule
    Top 10\% cut    & B & 4595 & 276063673 &  60079 \\
    Top 10\% cut    & F &  960 &  32400501 &  33751 \\
    Top 10\% cut    & M & 3445 & 130747836 &  37953 \\
    \midrule
    Log-transformed & B & 5266 & 557266412 & 10.00 \\
    Log-transformed & F & 1030 &  62493290 &  8.24 \\
    Log-transformed & M & 3704 & 242226228 &  8.58 \\
    \bottomrule
  \end{tabularx}
\end{table}

\begin{table}
  \centering
  \caption{Call time over different categorical features}
  \label{tab:calltime_over_all}
  \begin{tabularx}{.73\textwidth}{lccc}
    \toprule
    Feature & Number of customers & Total call time & Average call time \\
    \midrule
    Tariff plan 3 &  781 &  44811041  &  57376 \\
    Tariff plan 4 &   83 &  10575009  & 127410 \\
    Tariff plan 6 &  724 &  92214268  & 127368 \\
    Tariff plan 7 & 3564 & 489635465  & 137384 \\
    Tariff plan 8 & 4748 & 134616150  &  28352 \\
    \midrule
    Activation zone1 & 3507 & 288670845 & 82313 \\
    Activation zone2 & 3130 & 213438509 & 68191 \\
    Activation zone3 & 2342 & 197941555 & 84518 \\
    Activation zone4 &  921 &  71801024 & 77960 \\
    \midrule
    Activation channel 2 &  130 &  10643060 &  81870 \\
    Activation channel 3 &  408 &  36663577 &  89862 \\
    Activation channel 4 &   31 &   2369822 &  76446 \\
    Activation channel 5 & 7135 & 623961079 &  87451 \\
    Activation channel 6 &   47 &   4352853 &  92614 \\
    Activation channel 7 &  652 &  62777153 &  96284 \\
    Activation channel 8 &  111 &  12074180 & 108776 \\
    Activation channel 9 & 1386 &  19010209 &  13716 \\
    \midrule
    Value-added one: No & 7450 & 526620961 &  70687 \\
    Value-added one: Yes & 2450 & 245230972 & 100094 \\
    \midrule
    Value-added two: No & 9279 & 695404226 &  74944 \\
    Value-added two: Yes &  621 &  76447707 & 123104 \\
    \bottomrule
  \end{tabularx}
\end{table}

\subsubsection{Skewness}

Call time data is usually positively skewed, with many users having low usage and a small number of user with very high usage (e.g.,50.000 seconds). This is easily verifiable at a glance if we look at the density graph in figure \ref{fig:phone_call_density}. This effect can be mitigated by log-transforming the data, as shown in figure \ref{fig:phone_log_call_density}. Moreover, with the \textit{e1071} library I computed a skewness matrix for all features (appendix \ref{app:phone_skewness}): we can see that all monthly-based data (calls, call times, SMS, costs, etc) are highly positively skewed. To manage this problem we can either transform them (log, square root or Box-Cox transform), or use more robust models (such as random forest, Gamma/Poisson or quantile regression). Methods like linear. ridge or lasso regression, and even some non-parametric ones like KNN are not ideal. 

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{imgs/phone_y_hist.png}
        \caption{Call time density distribution}
        \label{fig:phone_call_density}
    \end{subfigure}
        \hspace{.05\linewidth}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{imgs/phone_log_y_hist.png}
        \caption{Call time density distribution (log-transformed)}
        \label{fig:phone_log_call_density}
    \end{subfigure}
    \caption{Call time density distributions for the \textit{phone\_train} dataset.}
    \Description{Graphs showing call times density.}
    \label{fig:phone_calltimesandamount}
\end{figure}

\subsubsection{Correlation Matrix}

Since \verb|cor| function only works on numerical data, I transformed all categorical with the \verb|model.matrix| function. We can infer the following: age is correlated to all call-related monthly features by roughly 25\%, first and second value-added services are correlated to some, not all, monthly features by roughly 10\%, and the only features that share significant correlation are the monthly call-related ones, for example in month four, outgoing call time and call expenses share a correlation of 97\%, while for the seventh month the correlation drop to 57\%, which is still significant. 

\subsubsection{Linear Regression Insights}

A preliminary analysis can be done by plotting a linear regression fit (figure \ref{fig:phone_4lnplots}). We can see that the data presents high heteroskedasticity, skewness and in general it is not normally distributed. There are also potential outliers and influential points that could distort the model, such as point 9853. As for features relevance (appendix \ref{app:phone_ln_features_relevance}), for the most part, relevant features are the one related to monthly calls. It is interesting to notice that the second month does not seem to count much, even thought it presents a similar amount of call data compared to the others.

Log-transforming the target variable yielded better results (figure \ref{fig:phone_4loglnplots}), but the above mentioned problems did not disappear. Feature relevance held some surprises (appendix \ref{app:phone_log_ln_features_relevance}), for example categorical features contributed much more to the model and only a small subset of monthly call data was relevant.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{imgs/4plots_ln.png}
        \caption{Linear regression fit plot}
        \label{fig:phone_4lnplots}
    \end{subfigure}
        \hspace{.05\linewidth}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{imgs/4plots_ln_log.png}
        \caption{Linear regression fit plot (log-transformed)}
        \label{fig:phone_4loglnplots}
    \end{subfigure}

    \caption{Linear regression fit plots for \textit{phone\_train} dataset.}
    \Description{Graphs showing linear regression fits}
\end{figure}

\subsubsection{Principal Components}

Data as-is can be transformed into 101 principal components, and the cumulative proportion with only two is approximately 0.45. To get to 99\% of explained variance we need 69 components. Plotting the first two components shows us, once again, signs of heteroskedasticity and non-normal data distribution, as shown in figure \ref{fig:phone_pca}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{imgs/PCAphone.png}
  \caption{Scatter Plot for call time for the \textit{phone\_train} dataset.}
    \Description{Scatterplot of multivariate call time.}
  \label{fig:phone_pca}
\end{figure}

\subsubsection{Premilinary Observations Conclusions}

Data shows high skewness and heteroskedasticity, and some features do not seem to contribute much to the model. We probably should try to filter out some data, for example SMS amount, transform monthly call data, for example with log or Box-Cox transforms, and use some robust regression methods, such as quantile or tree-based approaches.

\subsection{Modeling}

\subsubsection{Lasso Regression}

I used lasso regression with cross-validation as a preliminary diagnostic tool to check which features drop or transform.
First, I made a baseline prediction without transforming any value, with the following mean square error: $MSE = 18091358$. Not great, but expected since we saw in the preliminary observations many signs of non-linearity.

I then tried log-transforming the target value (with all features), which resulted in a massive improvement: $MSE = 6.200236$. 

Then, on the not transformed target, I tried different predictions with different data filtering, with the following results: filtering out incoming calls, SMS and calls to the call center yielded a 3\% improvement over the all-features model ($MSE = 18040253$). Putting calls to the call center back in worsened the model slightly ($MSE = 18080968$). I thus decided to leave these features filtered out permanently.

Then, I tired to filter out out either both or any between calls value and calls amount, which produced a worse performing model in all instances. Then, I filtered out non monthly features, and discovered in the end that the best result ($MSE = 18019454$) can be obtained with the following features \textbf{filtered out}: payment method, value-added service two, activation zone and activation channel (plus, of course, the previously mentioned incoming calls, SMS and calls to the call center).

I then tried to pre-process the monthly features, such as adding together peak and off-peak values or predict on the average monthly call times. All yielded worse results.

Lastly, wit the best performing filtering, I log-transformed only the target ($MSE = 6.253648$), only the monthly features ($MSE =20415157$), and then both, which ultimately yielded the bet result: $MSE = 4.289544$. It is worth nothing that log-transforming the target on the filtered dataset produce a worse performing fit than a model fitted on all data and log-transformed target. 

To summarize: the best performing model is obtained by filtering out payment method, value-added service two, activation zone, activation channel, incoming calls, SMS and calls to the call center, and by log-transforming the target and the remaining monthly call-related features.

The complete list of filtering and transformations can be seen in appendix \ref{app:phone_lasso_filtering}

\subsubsection{Random Forest}

Considering all of the above a model inherently robust to non-normally distributed data should yield better results. Random Forest is a good example of such a model. I still log-transforming both target and predictor since it showed good results, and fitting this type of model requires a lot of time. I run the model with the \verb|ranger| method on a \verb|tuneLength| of ten with cross-validation. Compared to the previous challenge I had to give up on LOOCV due to memory constraints.
The resulting best fit (figure \ref{fig:phone_randomorest},with $mtry=8$, variance as the split rule and minimum node size of five) yielded the best mean square error overall: $MSE = 3.947784$, which is why I ultimately used this model to predict the target value.

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{imgs/rf_plot.png}
  \caption{RMSE for random forest for the \textit{phone\_train} dataset.}
    \Description{RMSE for a random forest fit.}
  \label{fig:phone_randomforest}
\end{figure}

\subsection{Phone Users Conclusions}

The dataset was, in fact, highly not normally distributed and non-linear, so transforming the features greatly improved a linear fit such as lasso. Moreover, using a robust model yielded even better results, proving our hypothesis.